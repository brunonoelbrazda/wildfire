{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "from datetime import timedelta \n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Clustering Packages\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import classification_report\n",
    "from yellowbrick.classifier import ClassPredictionError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875d1b6",
   "metadata": {},
   "source": [
    "Wildfire database source : https://www.kaggle.com/rtatman/188-million-us-wildfires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3943d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up connection using path of the SQLite Database\n",
    "connection = sqlite3.connect('...')\n",
    "\n",
    "query = '''\n",
    "        SELECT *\n",
    "        FROM Fires\n",
    "        '''\n",
    "\n",
    "df = sql.read_sql(query, con = connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a175a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the discovery and containted dates as datetime\n",
    "\n",
    "query = '''\n",
    "SELECT datetime(DISCOVERY_DATE) AS DISCOVERY_DATE,\n",
    "datetime(CONT_DATE) AS CONT_DATE\n",
    "FROM Fires;\n",
    "'''\n",
    "date = sql.read_sql(query, con = connection)\n",
    "date['DISCOVERY_DATE'] = pd.to_datetime(date.DISCOVERY_DATE)\n",
    "date['CONT_DATE']=pd.to_datetime(date.CONT_DATE)\n",
    "df['DISCOVERY_DATE'] = date['DISCOVERY_DATE'].copy()\n",
    "df['CONT_DATE'] = date['CONT_DATE'].copy()\n",
    "\n",
    "#Looking at the fire_size feature\n",
    "\n",
    "df['FIRE_SIZE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that I didn't need for my model. \n",
    "# Primarily different Identifiers of Fires and Reporting Units\n",
    "df.drop(['FPA_ID','SOURCE_SYSTEM_TYPE','SOURCE_SYSTEM','NWCG_REPORTING_AGENCY',\n",
    "         'NWCG_REPORTING_UNIT_ID','NWCG_REPORTING_UNIT_NAME','SOURCE_REPORTING_UNIT',\n",
    "         'SOURCE_REPORTING_UNIT_NAME','LOCAL_FIRE_REPORT_ID','LOCAL_INCIDENT_ID',\n",
    "        'ICS_209_INCIDENT_NUMBER','ICS_209_NAME','MTBS_ID','Shape',\n",
    "        'FIRE_NAME','MTBS_FIRE_NAME','FIRE_CODE','COMPLEX_NAME','OBJECTID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ad0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d369792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling weekly to see how mean fire size varies over the time frame\n",
    "s = df['FIRE_SIZE'].copy()\n",
    "date = pd.concat([date,s], axis=1)\n",
    "date = date.set_index('DISCOVERY_DATE').copy()\n",
    "date.resample('W').mean().plot(figsize=(7,7),kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae102f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting all the fires\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x='LONGITUDE',y='LATITUDE',data=df)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5033b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investingating where null values are in the data\n",
    "sns.heatmap(df.isnull(),cbar=False,cmap='crest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking out correlations\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(),cbar=True,annot=True,cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ddd7d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding discovery month as a feature for predictions\n",
    "DISCOVERY_MONTH = []\n",
    "for i in df['DISCOVERY_DATE']:\n",
    "    month = int(i.strftime(\"%Y-%m-%d\")[5:7])\n",
    "    DISCOVERY_MONTH.append(month)\n",
    "df['DISCOVERY_MONTH'] = DISCOVERY_MONTH\n",
    "\n",
    "#Removing null values - due to my DBSCAN algorithm only being capable of \n",
    "# about 30,000 fires being inputted, I could drop incomplete rows and still\n",
    "# have more than enough rows for modelling. \n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = df.astype({'DISCOVERY_TIME': 'int64'}).copy()\n",
    "df = df.astype({'CONT_TIME': 'int64'}).copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the Time features - removing incomplete rows that were not\n",
    "# 4 characters long\n",
    "def cleaner(x):\n",
    "    x=str(x)\n",
    "    if len(x)<3:\n",
    "        return '!'\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df['DISCOVERY_TIME'] = df['DISCOVERY_TIME'].apply(cleaner).copy()\n",
    "df['CONT_TIME'] = df['CONT_TIME'].apply(cleaner).copy()\n",
    "\n",
    "df = df[df['DISCOVERY_TIME']!='!'].copy()\n",
    "df = df[df['CONT_TIME']!='!'].copy()\n",
    "\n",
    "def cleaner2(x):\n",
    "    try:\n",
    "        time = x[0]+x[1]+':'+x[2]+x[3]\n",
    "    except:\n",
    "        time = x[0]+':'+x[1]+x[2]\n",
    "    return time\n",
    "\n",
    "df['DISCOVERY_TIME'] = df['DISCOVERY_TIME'].apply(cleaner2).copy()\n",
    "df['CONT_TIME'] = df['CONT_TIME'].apply(cleaner2).copy()\n",
    "\n",
    "#Conversions of data type and creating one column combining \n",
    "# Date and Time\n",
    "\n",
    "df['DISCOVERY_DATE_AND_TIME'] = df['DISCOVERY_DATE'].astype(str).str.cat(df['DISCOVERY_TIME'].astype(str),sep=\" \")\n",
    "df['DISCOVERY_DATE_AND_TIME'] = pd.to_datetime(df['DISCOVERY_DATE_AND_TIME']).copy()\n",
    "\n",
    "df['CONT_DATE_AND_TIME'] = df['CONT_DATE'].astype(str).str.cat(df['CONT_TIME'].astype(str),sep=\" \")\n",
    "df['CONT_DATE_AND_TIME'] = pd.to_datetime(df['CONT_DATE_AND_TIME']).copy()\n",
    "\n",
    "#Creating the Duration Feature. This is the difference between when the\n",
    "#fire discovery date+time, and the contained date+time\n",
    "\n",
    "df['DURATION'] = df['CONT_DATE_AND_TIME'] - df['DISCOVERY_DATE_AND_TIME']\n",
    "\n",
    "def to_hours(x):\n",
    "    return x.total_seconds()/3600\n",
    "\n",
    "#conversion to hours\n",
    "df['DURATION'] = df['DURATION'].apply(to_hours).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852f001",
   "metadata": {},
   "source": [
    "The Dataframe is now ready to be used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeaf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('wildfire_csv_withduration.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa7139",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862bfba",
   "metadata": {},
   "source": [
    "The code below was used twice, as I carried out clustering at first using 30,000 of the most recent fires, as well as a second time using 30,000 of the older ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Long, Lat\n",
    "#FOD_ID Unique identifier to make joining back to original DF easier\n",
    "fire_locations = df[['FOD_ID','LATITUDE','LONGITUDE']].copy()\n",
    "fire_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6163e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fire_locations.head(30000).copy()\n",
    "#X = fire_locations.tail(30000).copy()\n",
    "\n",
    "#Building the distance_matrix for DBSCAN\n",
    "\n",
    "X['latitude_rad'] = X['LATITUDE']*np.pi/180\n",
    "X['longitude_rad'] = X['LONGITUDE']*np.pi/180\n",
    "distance_matrix = pairwise.haversine_distances(X.loc[:,\n",
    "['latitude_rad', 'longitude_rad']])*6371"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb3fd4",
   "metadata": {},
   "source": [
    "The two parameters that need to be adjusted for DBSCAN are epsilon and minimum samples. \n",
    "EPS is the maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "Minimum Samples, as the name suggests, is the number of samples in a neighborhood for a point to be considered as a core point.\n",
    "By trying different combinations of these I was able to find a suitable number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40990b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=300, min_samples=2000, metric='precomputed')\n",
    "y_db = db.fit_predict(distance_matrix)  \n",
    "X['cluster'] = y_db\n",
    "print(len(X.cluster.unique()))\n",
    "X.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the clusters\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X['LONGITUDE'], X['LATITUDE'], c=X['cluster'],\n",
    "            cmap='spring', s=40)\n",
    "plt.xlabel('Longitude', fontsize=24)\n",
    "plt.ylabel('Latitude', fontsize=24)\n",
    "plt.title('DBScan clustering (3 clusters)', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the original df with the clusters df\n",
    "df3 = pd.merge(df, X, on='FOD_ID', how='inner')\n",
    "#Selecting one of the clusters\n",
    "c2 = df3[df3['cluster']==-1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae221fb",
   "metadata": {},
   "source": [
    "![3Clusters](/visuals/3clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5985fb",
   "metadata": {},
   "source": [
    "![5Clusters](/visuals/5clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61087491",
   "metadata": {},
   "source": [
    "# Adding Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f4c1e",
   "metadata": {},
   "source": [
    "To add historic weather data I used the Virtual Crossing API - the response contained a large amount of weather data for the latitude,longitude, and date provided.\n",
    "The code below was used several times for all the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bdabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2['DISCOVERY_DATE'] = pd.to_datetime(c2['DISCOVERY_DATE'])\n",
    "c22 = c2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47eaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists which I append the humidity and temperature to.\n",
    "humidityc2 = []\n",
    "tempsc2 = []\n",
    "\n",
    "for i in range(0,7442):\n",
    "    lat = round(c22['LATITUDE_x'][i],4)\n",
    "    long = round(c22['LONGITUDE_x'][i],4)\n",
    "    date = c22['DISCOVERY_DATE'][i].strftime(\"%Y-%m-%d\")\n",
    "    year = int(c22['DISCOVERY_DATE'][i].strftime(\"%Y-%m-%d\")[0:4])\n",
    "    month = int(c22['DISCOVERY_DATE'][i].strftime(\"%Y-%m-%d\")[5:7])\n",
    "    day = int(c22['DISCOVERY_DATE'][i].strftime(\"%Y-%m-%d\")[8:10])\n",
    "    \n",
    "    url = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat}%2C{long}/{year}-{month}-{day}?unitGroup=metric&key={apikey}&include=obs'\n",
    "\n",
    "    response = requests.request(\"GET\",url)\n",
    "    response_dict = json.loads(response.text)\n",
    "    tempsc2.append(response_dict['days'][0]['temp'])\n",
    "    humidityc2.append(response_dict['days'][0]['humidity'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2['HUMIDITY'] = humidityc2\n",
    "c2['TEMP'] = tempsc2\n",
    "c2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b12dc",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57146e7e",
   "metadata": {},
   "source": [
    "I carried out the same steps for each cluster:\n",
    "Features were selected, and categorical variables were dummified.The data was standardized, and the target was selected. Following a training - validation split, using the standard 80-20 ratio, I fit a decision tree classifier on the training data. The mean cross validation score on the test set was then obtained. In the majority of cases the score was only marginally above baseline, however in the case of two cases the score achieved was far above baseline. For those two clusters I went further by applying grid search to optimize a random forest classifier. I then obtained a confusion matrix as well as other key metrics to evaluate my model. Finally, the feature importances were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1704cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying the baseline accuracy for the model\n",
    "c2['FIRE_SIZE_CLASS'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = c2[['LATITUDE_x','LONGITUDE_x','STAT_CAUSE_DESCR','DURATION',\n",
    "       'OWNER_DESCR','COUNTY','STATE','DISCOVERY_DOY','CONT_DOY',\n",
    "        'HUMIDITY','TEMP','DISCOVERY_MONTH']].copy()\n",
    "X = pd.get_dummies(X,columns=['STAT_CAUSE_DESCR','OWNER_DESCR','DISCOVERY_MONTH',\n",
    "                              'DISCOVERY_DOY','CONT_DOY','COUNTY','STATE'],drop_first=True).copy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = c2['FIRE_SIZE_CLASS'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    stratify=y, \n",
    "    test_size=0.2, \n",
    "    random_state=1)\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(cross_val_score(model, X_test, y_test, cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc718bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "                           n_estimators=100,\n",
    "                           random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print(cross_val_score(model, X_test, y_test, cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ce828",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_true, y_pred, figsize=(6,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8389fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "target_names = ['Fire Size A', 'Fire Size B']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['A','B']\n",
    "\n",
    "visualizer = ClassPredictionError(\n",
    "    RandomForestClassifier(random_state=1, n_estimators=100), classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d52622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(y_true, y_pred, labels=None, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1474ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test = model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", len(model.classes_)))\n",
    "\n",
    "skplt.metrics.plot_precision_recall(y_test, probabilities_test, cmap=cmap,figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, probabilities_test, cmap=cmap,figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5253acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = c2[['LATITUDE_x','LONGITUDE_x','STAT_CAUSE_DESCR','DURATION',\n",
    "       'OWNER_DESCR','COUNTY','STATE','DISCOVERY_DOY','CONT_DOY',\n",
    "        'HUMIDITY','TEMP','DISCOVERY_MONTH']].copy()\n",
    "X = pd.get_dummies(X,columns=['STAT_CAUSE_DESCR','OWNER_DESCR','DISCOVERY_MONTH',\n",
    "                              'DISCOVERY_DOY','CONT_DOY','COUNTY','STATE'],drop_first=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03411b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(6).plot(kind='barh')\n",
    "plt.title(\"Top 6 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b0fa6",
   "metadata": {},
   "source": [
    "### Principle Comoponent Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f924b0",
   "metadata": {},
   "source": [
    "With a mean CV score noticeably higher than baseline, PCA proved to be \n",
    "an effective way of reducing dimensionality. With that being said,\n",
    "being unable to extract feature importances made it not particularly \n",
    "useful for acheiving my goal, although it was nice to see 3 components retain the original signal in the data so well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16540e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 components captured over 97% of the variance.\n",
    "c2_pca = PCA(n_components=5)\n",
    "c2_pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained Variance Ratios:')\n",
    "c2_pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification step\n",
    "c2_pcs = c2_pca.transform(X)\n",
    "print(np.allclose(np.corrcoef(c2_pcs.T), np.eye(len(c2_pcs.T))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_pcs = pd.DataFrame(c2_pca.transform(X), \n",
    "                        columns=[f'PC_{i+1}' for i in range(c2_pca.n_components_)])\n",
    "c2_pcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e586d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest on PCA, seeing how well the model performs with 3 components\n",
    "X = c2_pcs[['PC_1','PC_2','PC_3']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X)\n",
    "\n",
    "y = c2['FIRE_SIZE_CLASS'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    stratify=y, \n",
    "    test_size=0.2, \n",
    "    random_state=1)\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "                           n_estimators=100,\n",
    "                           random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(model, X_test, y_test, cv=5).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
